# ğŸ§  CODEX â€“ AI Debug Action Predictor

Codex est une **IA fine-tunÃ©e sur le modÃ¨le T5** qui **prÃ©dit automatiquement lâ€™action de correction** Ã  effectuer lorsquâ€™une erreur de code est dÃ©tectÃ©e.  
> Exemple :  
> "missing colon in function definition" â†’ `add_colon_to_function`

---

## ğŸš€ Objectif

CrÃ©er un **assistant IA intelligent** capable dâ€™identifier le type dâ€™erreur rencontrÃ©e dans du code, et de **suggÃ©rer automatiquement la correction la plus appropriÃ©e**.

---

## ğŸ“¦ FonctionnalitÃ©s

- ğŸ”® PrÃ©diction d'action correctrice Ã  partir dâ€™un message dâ€™erreur
- ğŸ§  ModÃ¨le basÃ© sur `t5-small` fine-tunÃ© sur plus de 200 cas
- ğŸ“š DonnÃ©es d'entraÃ®nement sous format `.jsonl`
- ğŸ” RÃ©-entraÃ®nement possible et rapide via HuggingFace
- âœ… Ã‰valuation automatique via `test_model.py`

---

## ğŸ“‚ Structure du projet
CODEX-AI/ â”œâ”€â”€ train_model.py # Script d'entraÃ®nement du modÃ¨le â”œâ”€â”€ test_model.py # Script de test des prÃ©dictions â”œâ”€â”€ dataset.jsonl # DonnÃ©es d'entraÃ®nement (erreurs et actions) â”œâ”€â”€ debug_model/ # RÃ©pertoire contenant les checkpoints entraÃ®nÃ©s â”‚ â””â”€â”€ checkpoint-xxx/ # Derniers poids enregistrÃ©s â””â”€â”€ README.md # Ce fichier de documentation

---

## âš™ï¸ Installation & Setup

### 1. Clone ce dÃ©pÃ´t :
```bash
git clone https://github.com/purplekan/CODEX-AI.git
cd CODEX-AI
```
### 2. CrÃ©e un environnement virtuel :
```bash
python -m venv .venv
# Sur Windows :
.venv\Scripts\activate
# Sur Mac/Linux :
source .venv/bin/activate
```
### 3. Installe les dÃ©pendances

Installe les dÃ©pendances :

```bash
pip install -r requirements.txt
```
Si requirements.txt nâ€™existe pas encore, tu peux installer manuellement :

```bash
pip install transformers datasets
```

## ğŸ§ª Tester le modÃ¨le
```bash
python test_model.py
```
Le script affiche :
- ğŸ” Lâ€™entrÃ©e (ex. : "function not defined")
- âœ… La sortie attendue (ex. : define_function)
- ğŸ”® La prÃ©diction du modÃ¨le
- âœ”ï¸ Ou âŒ selon la correspondance

## ğŸ§  EntraÃ®ner le modÃ¨le
```bash
python train_model.py
```
Cela :
- Charge les donnÃ©es depuis dataset.jsonl
- EntraÃ®ne le modÃ¨le T5
- Sauvegarde les checkpoints dans debug_model/

Tu peux augmenter les performances en enrichissant dataset.jsonl avec plus de cas.

## ğŸ“ˆ Exemple de donnÃ©es d'entraÃ®nement (dataset.jsonl)
```json
{"input": "function not defined", "output": "define_function"}
{"input": "missing colon in function definition", "output": "add_colon_to_function"}
{"input": "list index out of range", "output": "check_list_index"}
```

ğŸ”§ Conseils pour de meilleurs rÃ©sultats
- EntraÃ®ne sur un nombre plus grand dâ€™exemples
- Augmente num_train_epochs dans TrainingArguments
- Utilise num_beams, temperature, et repetition_penalty pour des prÃ©dictions plus prÃ©cises

## ğŸ“š Ressources utilisÃ©es

- ğŸ¤— [HuggingFace Transformers](https://github.com/huggingface/transformers)
- ğŸ§  ModÃ¨le `t5-small`
- ğŸ” Fine-tuning basÃ© sur `Trainer` + `Datasets`

------

## ğŸ›¡ï¸ Licence

Ce projet est sous licence **MIT**.

------

## ğŸ§  Ã€ propos

Ce projet a Ã©tÃ© co-dÃ©veloppÃ© par [purplekan](https://github.com/purplekan) et son copilote IA ChatGPT ğŸ¤
 Dans une vision partagÃ©e : **entraÃ®ner, tester, progresser.**

------

> "Ce projet est la premiÃ¨re pierre dâ€™un Ã©difice plus grand : des IA collaboratives, spÃ©cialisÃ©es, et redoutables."











